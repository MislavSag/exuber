f7d7eea22b78efd01b1da8d5f8f2cbbe81322293 MislavSag Mon May 26 16:37:15 2025 +0200 lubridate
diff --git a/image.def b/image.def
index 25e292d..a20f3ba 100644
--- a/image.def
+++ b/image.def
@@ -14,6 +14,7 @@ From: r-base:4.5.0
   R --slave -e 'install.packages("remotes")'
   R --slave -e 'remotes::install_github("MislavSag/finfeatures")'
   R --slave -e 'install.packages("exuber")'
+  R --slave -e 'install.packages("lubridate")'
 
 %runscript
   # Script to run when the container is executed; passes commands to Rscript
diff --git a/padobran2indicators.R b/padobran2indicators.R
index 05876a2..64ae1a3 100644
--- a/padobran2indicators.R
+++ b/padobran2indicators.R
@@ -1,4 +1,5 @@
 library(data.table)
+library(lubridate)
 
 
 # SETUP -------------------------------------------------------------------

7cd455343e97ae42b4d9b29e5dcc657bd100db88 MislavSag Mon May 26 15:03:26 2025 +0200 yes
diff --git a/padobran2indicators.R b/padobran2indicators.R
index 4e10419..05876a2 100644
--- a/padobran2indicators.R
+++ b/padobran2indicators.R
@@ -31,7 +31,7 @@ if (interactive()) {
 if (interactive()) {
   files = list.files(file.path(PATH, "predictors"), full.names = TRUE)
 } else {
-  files = list.files("predictors", full.names = TRUE)[i]
+  files = list.files("predictors", full.names = TRUE)
 }
 
 # Import predictors in chunks Predictors

5364794114d8a81fdd2bc46ef8dc3d5168562baa MislavSag Mon May 26 14:46:47 2025 +0200 yes
diff --git a/padobran2indicators.R b/padobran2indicators.R
index 0addbd5..4e10419 100644
--- a/padobran2indicators.R
+++ b/padobran2indicators.R
@@ -8,7 +8,7 @@ if (interactive()) {
 }
 
 # Create indicators directory if it doesn't exist
-if (!interactive()) {
+if (interactive()) {
   if (!dir.exists(file.path(PATH, "indicators"))) {
     dir.create(file.path(PATH, "indicators"))
   }

709bbf52cfe92cb62ad98e58c82e68a7d129cb69 MislavSag Mon May 26 14:20:51 2025 +0200 yes
diff --git a/padobran2indicators.R b/padobran2indicators.R
index 9367c4c..0addbd5 100644
--- a/padobran2indicators.R
+++ b/padobran2indicators.R
@@ -8,8 +8,10 @@ if (interactive()) {
 }
 
 # Create indicators directory if it doesn't exist
-if (!dir.exists(file.path(PATH, "indicators"))) {
-  dir.create(file.path(PATH, "indicators"))
+if (!interactive()) {
+  if (!dir.exists(file.path(PATH, "indicators"))) {
+    dir.create(file.path(PATH, "indicators"))
+  }
 }
 
 # DATA --------------------------------------------------------------------

95ac359fbdfb8fc7ee299dedcad09f0fa01a5ec8 MislavSag Mon May 26 14:09:13 2025 +0200 indicators
diff --git a/insample.R b/insample.R
new file mode 100644
index 0000000..1234044
--- /dev/null
+++ b/insample.R
@@ -0,0 +1,307 @@
+library(finutils)
+library(fastverse)
+library(lubridate)
+library(PerformanceAnalytics)
+library(TTR)
+library(ggplot2)
+library(roll)
+
+
+# SETUP -------------------------------------------------------------------
+# Globals
+PATH = "D:/strategies/exuber"
+
+
+# DATA --------------------------------------------------------------------
+# Prices
+files = list.files(file.path(PATH, "prices"), full.names = TRUE)
+prices = lapply(files, fread)
+prices = prices[, .(symbol, date, open, close)]
+
+# Get data for SPY
+spy = qc_hour(
+  file_path = "F:/lean/data/stocks_hour.csv",
+  symbols = "spy",
+  duplicates = "fast"
+)
+
+# Import predictors in chunks Predictors
+files = list.files(file.path(PATH, "predictors"), full.names = TRUE)
+fread(files[1])
+dt = lapply(files, fread)
+dt = rbindlist(dt, fill= TRUE)
+dt[, date := with_tz(date, tzone = "America/New_York")]
+setorder(dt, symbol, date)
+
+# Market cap
+mcap = qc_daily(
+  file_path = "F:/lean/data/stocks_daily.csv",
+  duplicates = "none",
+  symbols = dt[, unique(symbol)],
+  profiles_fmp = TRUE,
+  fmp_api_key = Sys.getenv("APIKEY")
+)
+
+
+# PREPARE DATA ------------------------------------------------------------
+# Merge prices and predictors
+dt = merge(dt, prices, by = c("symbol", "date"), all.x = TRUE, all.y = FALSE)
+
+# Delete prices to save memory
+# rm(prices)
+# gc()
+
+# Calculate returns
+setorder(dt, symbol, date)
+dt[, returns := close / shift(close) - 1, by = symbol]
+
+# Remove missing values
+dt = na.omit(dt)
+
+
+# INDICATORS --------------------------------------------------------------
+# Median aggreagation
+radf_vars = colnames(dt)[grepl("exuber", colnames(dt))]
+indicators_median = dt[, lapply(.SD, median, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_median, radf_vars, paste0("median_", radf_vars))
+
+# Standard deviation aggregation
+indicators_sd = dt[, lapply(.SD, sd, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_sd, radf_vars, paste0("sd_", radf_vars))
+
+# Mean aggregation
+indicators_mean = dt[, lapply(.SD, mean, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_mean, radf_vars, paste0("mean_", radf_vars))
+
+# Sum aggreagation
+indicators_sum = dt[, lapply(.SD, sum, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_sum, radf_vars, paste0("sum_", radf_vars))
+
+# # Quantile aggregations
+# indicators_q = dt[, lapply(.SD, quantile, probs = 0.99, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+# setnames(indicators_q, radf_vars, paste0("q99_", radf_vars))
+# indicators_q97 = dt[, lapply(.SD, quantile, probs = 0.97, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+# setnames(indicators_q97, radf_vars, paste0("q97_", radf_vars))
+# indicators_q95 = dt[, lapply(.SD, quantile, probs = 0.95, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+# setnames(indicators_q95, radf_vars, paste0("q95_", radf_vars))
+#
+# # Skew aggregation
+# indicators_skew = dt[, lapply(.SD, skewness, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+# setnames(indicators_skew, radf_vars, paste0("skew_", radf_vars))
+#
+# # Kurtosis aggregation
+# indicators_kurt = dt[, lapply(.SD, kurtosis, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+# setnames(indicators_kurt, radf_vars, paste0("kurtosis_", radf_vars))
+
+# Merge indicators
+indicators = Reduce(
+  function(x, y)
+    merge(
+      x,
+      y,
+      by = "date",
+      all.x = TRUE,
+      all.y = TRUE
+    ),
+  list(
+    indicators_sd,
+    indicators_median,
+    indicators_sum,
+    indicators_mean
+    # indicators_q,
+    # indicators_q97,
+    # indicators_q95,
+    # indicators_skew,
+    # indicators_kurt
+  )
+)
+setorder(indicators, date)
+
+# Remove missing values
+indicators = na.omit(indicators)
+
+# visualize
+colnames(indicators)
+plot(as.xts.data.table(indicators[, .(date, sd_exuber_200_0_sadf_log)]))
+plot(as.xts.data.table(indicators[, .(date, median_exuber_200_0_sadf_log)]))
+plot(as.xts.data.table(indicators[, .(date, mean_exuber_200_0_sadf_log)]))
+# plot(as.xts.data.table(indicators[, .(date, q97_exuber_200_0_sadf_log)]))
+# plot(as.xts.data.table(indicators[, .(date, q99_exuber_200_0_sadf_log)]))
+# plot(as.xts.data.table(indicators[, .(date, skew_exuber_200_0_sadf_log)]))
+# plot(as.xts.data.table(indicators[, .(date, kurtosis_exuber_200_0_sadf_log)]))
+plot(as.xts.data.table(indicators[, .(date, EMA(sd_exuber_200_0_sadf_log, n = 7 * 10))]))
+plot(as.xts.data.table(indicators[, .(date, EMA(sd_exuber_200_0_sadf_log, n = 7 * 10))])["2025"])
+plot(as.xts.data.table(indicators[, .(date, EMA(sd_exuber_200_0_sadf_log, n = 7 * 10))])["2022"])
+plot(as.xts.data.table(indicators[, .(date, EMA(mean_exuber_200_0_sadf_log, n = 7 * 10))])["2025"])
+plot(as.xts.data.table(indicators[, .(date, EMA(mean_exuber_200_0_sadf_log, n = 7 * 10))])["2022"])
+plot(as.xts.data.table(indicators[, .(date, EMA(median_exuber_200_0_sadf_log, n = 7 * 10))])["2025"])
+plot(as.xts.data.table(indicators[, .(date, EMA(sd_exuber_800_0_sadf_log, n = 7 * 10))])["2025"])
+plot(as.xts.data.table(indicators[, .(date, EMA(sd_exuber_800_1_sadf_log, n = 7 * 10))])["2025"])
+
+# Merge SPY and indicators
+backtest_data = indicators[spy, on = "date"]
+
+# Add trend predictor
+backtest_data[, trend := EMA(close, n = 7 * 10) > EMA(close, n = 7 * 10 * 50)]
+
+# Lag all indicators!
+# cols_exuber = colnames(backtest_data)[grep("exuber", colnames(backtest_data))]
+# backtest_data[, (cols_exuber) := shift(.SD), .SDcols = cols_exuber]
+
+# Remove missing values
+backtest_data = na.omit(backtest_data)
+setorder(backtest_data, date)
+
+
+# RESEARCH ----------------------------------------------------------------
+# Hour, daily and week Returns across deciles
+backtest_data[, let(
+  target_hour = shift(close, type = "lead", n = 1L) / close - 1,
+  target_daily = shift(close, type = "lead", n = 7L) / close - 1,
+  target_week = shift(close, type = "lead", n = 10L * 7L) / close - 1
+)]
+
+# Plot returns across deciles
+cols_choose = cols_exuber[grepl("800_1", cols_exuber)]
+var_ = "sd_exuber_800_1_gsadf_log"
+backtest_data[, .(date, target_hour, decile = dplyr::ntile(x, 10)), env = list(x = var_)] |>
+  _[, mean(target_hour), by = decile] |>
+  ggplot(aes(x = decile, y = V1)) +
+  geom_col() +
+  geom_hline(yintercept = 0, linetype = "dashed") +
+  labs(
+    title = paste0("Hour returns across deciles of ", var_),
+    x = "Decile",
+    y = "Mean Hourly Return"
+  ) +
+  theme_minimal()
+backtest_data[, .(date, target_daily, decile = dplyr::ntile(x, 10)), env = list(x = var_)] |>
+  _[, mean(target_daily), by = decile] |>
+  ggplot(aes(x = decile, y = V1)) +
+  geom_col() +
+  geom_hline(yintercept = 0, linetype = "dashed") +
+  labs(
+    title = paste0("Hour returns across deciles of ", var_),
+    x = "Decile",
+    y = "Mean Hourly Return"
+  ) +
+  theme_minimal()
+backtest_data[, .(date, target_week, decile = dplyr::ntile(x, 10)), env = list(x = var_)] |>
+  _[, mean(target_week), by = decile] |>
+  ggplot(aes(x = decile, y = V1)) +
+  geom_col() +
+  geom_hline(yintercept = 0, linetype = "dashed") +
+  labs(
+    title = paste0("Hour returns across deciles of ", var_),
+    x = "Decile",
+    y = "Mean Hourly Return"
+  ) +
+  theme_minimal()
+backtest_data[, .(date, target_week, decile = dplyr::ntile(x, 10)), env = list(x = var_)] |>
+  _[, mean(target_week), by = decile] |>
+  ggplot(aes(x = decile, y = V1)) +
+  geom_col() +
+  geom_hline(yintercept = 0, linetype = "dashed") +
+  labs(
+    title = paste0("Hour returns across deciles of ", var_),
+    x = "Decile",
+    y = "Mean Hourly Return"
+  ) +
+  theme_minimal()
+backtest_data[trend == TRUE, .(date, target_week, decile = dplyr::ntile(x, 10)), env = list(x = var_)] |>
+  _[, mean(target_week), by = decile] |>
+  ggplot(aes(x = decile, y = V1)) +
+  geom_col() +
+  geom_hline(yintercept = 0, linetype = "dashed") +
+  labs(
+    title = paste0("Hour returns across deciles of ", var_),
+    x = "Decile",
+    y = "Mean Hourly Return"
+  ) +
+  theme_minimal()
+backtest_data[trend == FALSE, .(date, target_week, decile = dplyr::ntile(x, 10)), env = list(x = var_)] |>
+  _[, mean(target_week), by = decile] |>
+  ggplot(aes(x = decile, y = V1)) +
+  geom_col() +
+  geom_hline(yintercept = 0, linetype = "dashed") +
+  labs(
+    title = paste0("Hour returns across deciles of ", var_),
+    x = "Decile",
+    y = "Mean Hourly Return"
+  ) +
+  theme_minimal()
+
+# Simple backtest
+dt_ = backtest_data[, .(date, target_hour, x, close), env = list(x = var_)]
+dt_[, x := EMA(x, 7*2), env = list(x = var_)]
+dt_[, trend := close / shift(close, n = 7 * 5) - 1]
+dt_[, let(
+  q90 = roll_quantile(x, width = length(x), p = 0.90, min_obs = 100L),
+  q80 = roll_quantile(x, width = length(x), p = 0.80, min_obs = 100L),
+  q70 = roll_quantile(x, width = length(x), p = 0.70, min_obs = 100L),
+  q60 = roll_quantile(x, width = length(x), p = 0.60, min_obs = 100L),
+  q50 = roll_quantile(x, width = length(x), p = 0.50, min_obs = 100L),
+  q40 = roll_quantile(x, width = length(x), p = 0.40, min_obs = 100L),
+  q30 = roll_quantile(x, width = length(x), p = 0.30, min_obs = 100L),
+  q20 = roll_quantile(x, width = length(x), p = 0.20, min_obs = 100L),
+  q10 = roll_quantile(x, width = length(x), p = 0.10, min_obs = 100L)
+), env = list(x = var_)]
+max_leverage = 2
+# dt_[, signal := fcase(
+#   x < q10 & trend > 0, max_leverage,
+#   x < q20 & trend > 0, max_leverage * 0.85,
+#   x < q30 & trend > 0, max_leverage * 0.7,
+#   x < q40 & trend > 0, max_leverage * 0.55,
+#   x < q50 & trend > 0, max_leverage * 0.40,
+#   x < q60 & trend > 0, max_leverage * 0.25,
+#   x < q70 & trend > 0, max_leverage * 0.2,
+#   x < q80 & trend > 0, max_leverage * 0.05,
+#   x < q90 & trend > 0, max_leverage * 0,
+#   x > q90 & trend > 0, max_leverage * 0,
+#   default = 0
+# ), env = list(x = var_)]
+dt_[, signal := fcase(
+  x < q10, max_leverage,
+  x < q20, max_leverage * 0.85,
+  x < q30, max_leverage * 0.7,
+  x < q40, max_leverage * 0.55,
+  x < q50, max_leverage * 0.40,
+  x < q60, max_leverage * 0.25,
+  x < q70, max_leverage * 0.2,
+  x < q80, max_leverage * 0.05,
+  x < q90, max_leverage * 0,
+  x > q90, max_leverage * 0,
+  default = 0
+  ), env = list(x = var_)]
+dt_[, signal := shift(signal, type = "lag", n = 1L)]
+dt_ = na.omit(dt_)
+strategy = as.xts.data.table(dt_[, .(date,
+                                     strategy = signal * target_hour,
+                                     benchmark = target_hour)])
+strategy_daily = apply.daily(strategy[, 1], function(x) {prod(1 + x) - 1})
+strategy_daily$benchmark = apply.daily(strategy[, 2], function(x) {prod(1 + x) - 1})
+charts.PerformanceSummary(strategy_daily)
+SharpeRatio.annualized(strategy_daily)
+Return.annualized(strategy_daily)
+# charts.PerformanceSummary(strategy_daily["2025"])
+
+# Choose multiple predictors
+sd_predictors = colnames(backtest_data)[grepl("sd", colnames(backtest_data))]
+dt_ = backtest_data[, .SD, .SD = c("date", "target_hour", "close", sd_predictors)]
+dt_[, (sd_predictors) := lapply(.SD, function(x) (EMA(x, 7) + EMA(x, 7*5) + EMA(x, 7*10)) / 3),
+    .SDcols = sd_predictors]
+qs = dt_[, lapply(.SD, function(x) {
+  q90 = roll_quantile(x, width = length(x), p = 0.90, min_obs = 100L)
+  q80 = roll_quantile(x, width = length(x), p = 0.80, min_obs = 100L)
+  q70 = roll_quantile(x, width = length(x), p = 0.70, min_obs = 100L)
+  q60 = roll_quantile(x, width = length(x), p = 0.60, min_obs = 100L)
+  q50 = roll_quantile(x, width = length(x), p = 0.50, min_obs = 100L)
+  q40 = roll_quantile(x, width = length(x), p = 0.40, min_obs = 100L)
+  q30 = roll_quantile(x, width = length(x), p = 0.30, min_obs = 100L)
+  q20 = roll_quantile(x, width = length(x), p = 0.20, min_obs = 100L)
+  q10 = roll_quantile(x, width = length(x), p = 0.10, min_obs = 100L)
+  cbind(q90, q80, q70, q60, q50, q40, q30, q20, q10)
+}), .SDcols = sd_predictors]
+qs[, 1:6]
+
+
diff --git a/padobran2indicators.R b/padobran2indicators.R
new file mode 100644
index 0000000..9367c4c
--- /dev/null
+++ b/padobran2indicators.R
@@ -0,0 +1,84 @@
+library(data.table)
+
+
+# SETUP -------------------------------------------------------------------
+# Globals
+if (interactive()) {
+  PATH = "D:/strategies/exuber"
+}
+
+# Create indicators directory if it doesn't exist
+if (!dir.exists(file.path(PATH, "indicators"))) {
+  dir.create(file.path(PATH, "indicators"))
+}
+
+# DATA --------------------------------------------------------------------
+# Check columns
+# colnames(fread(list.files(file.path(PATH, "predictors"), full.names = TRUE)[1]))
+
+# Current column
+# if (interactive()) {
+#   i = 1L
+# } else {
+#   i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+# }
+# i = i + 2L
+# cols = c(1, 2, i)
+
+# Get files
+if (interactive()) {
+  files = list.files(file.path(PATH, "predictors"), full.names = TRUE)
+} else {
+  files = list.files("predictors", full.names = TRUE)[i]
+}
+
+# Import predictors in chunks Predictors
+system.time({
+  dt = lapply(files, fread) # , select = cols
+})
+format(object.size(dt), units = "auto")
+dt = rbindlist(dt, fill= TRUE)
+dt[, date := with_tz(date, tzone = "America/New_York")]
+setorder(dt, symbol, date)
+
+# Median aggreagation
+radf_vars = colnames(dt)[grepl("exuber", colnames(dt))]
+indicators_median = dt[, lapply(.SD, median, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_median, radf_vars, paste0("median_", radf_vars))
+
+# Standard deviation aggregation
+indicators_sd = dt[, lapply(.SD, sd, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_sd, radf_vars, paste0("sd_", radf_vars))
+
+# Mean aggregation
+indicators_mean = dt[, lapply(.SD, mean, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_mean, radf_vars, paste0("mean_", radf_vars))
+
+# Sum aggreagation
+indicators_sum = dt[, lapply(.SD, sum, na.rm = TRUE), by = c('date'), .SDcols = radf_vars]
+setnames(indicators_sum, radf_vars, paste0("sum_", radf_vars))
+
+# Merge indicators
+indicators = Reduce(
+  function(x, y)
+    merge(
+      x,
+      y,
+      by = "date",
+      all.x = TRUE,
+      all.y = TRUE
+    ),
+  list(
+    indicators_sd,
+    indicators_median,
+    indicators_sum,
+    indicators_mean
+  )
+)
+setorder(indicators, date)
+
+# Remove missing values
+indicators = na.omit(indicators)
+
+# Save indicators
+fwrite(indicators, "indicators.csv")
diff --git a/padobran2indicators.sh b/padobran2indicators.sh
new file mode 100644
index 0000000..4ef2166
--- /dev/null
+++ b/padobran2indicators.sh
@@ -0,0 +1,11 @@
+#!/bin/bash
+
+#PBS -N indicators
+#PBS -l ncpus=1
+#PBS -l mem=130GB
+#PBS -o logs
+#PBS -j oe
+
+cd ${PBS_O_WORKDIR}
+
+apptainer run image.sif padobran2indicators.R

c3b87f8b09f62be5a04bff24902e68ac2bb702e3 MislavSag Thu May 22 15:22:32 2025 +0200 predictors
diff --git a/padobran_predictors.R b/padobran_predictors.R
index 2efc2a5..6c3a0ae 100644
--- a/padobran_predictors.R
+++ b/padobran_predictors.R
@@ -1,5 +1,3 @@
-options(progress_enabled = FALSE)
-
 library(data.table)
 library(finfeatures)
 
@@ -44,9 +42,9 @@ ohlcv = Ohlcv$new(ohlcv[, .(symbol, date, open, high, low, close, volume)],
 exuber_init = RollingExuber$new(
   windows = c(100, 200, 400, 800),
   workers = 1L,
-  at = 1:220, # nrow(ohlcv$X),
+  at = 1:nrow(ohlcv$X),
   lag = 0L,
-  exuber_lag = c(1L)
+  exuber_lag = c(0L, 1L)
 )
 exuber = exuber_init$get_rolling_features(ohlcv, log_prices = TRUE)
 fwrite(exuber, file.path(PATH_PREDICTORS, paste0(symbol_i, ".csv")))

d63746b1dfc9743bec49ceded0fb0f4e8d2e3b13 MislavSag Thu May 22 14:59:13 2025 +0200 predictors
diff --git a/image.def b/image.def
index d9932d6..25e292d 100644
--- a/image.def
+++ b/image.def
@@ -1,5 +1,5 @@
 Bootstrap: docker
-From: r-base:4.4.4
+From: r-base:4.5.0
 
 %post
 

bd26f6876f9521d71cbca30cbf3f77820823f194 MislavSag Thu May 22 14:56:10 2025 +0200 predictors
diff --git a/.gitignore b/.gitignore
new file mode 100644
index 0000000..221b531
--- /dev/null
+++ b/.gitignore
@@ -0,0 +1,5 @@
+.Rproj.user
+.Rhistory
+.RData
+.Ruserdata
+.Renviron
diff --git a/exuber.Rproj b/exuber.Rproj
new file mode 100644
index 0000000..e83436a
--- /dev/null
+++ b/exuber.Rproj
@@ -0,0 +1,16 @@
+Version: 1.0
+
+RestoreWorkspace: Default
+SaveWorkspace: Default
+AlwaysSaveHistory: Default
+
+EnableCodeIndexing: Yes
+UseSpacesForTab: Yes
+NumSpacesForTab: 2
+Encoding: UTF-8
+
+RnwWeave: Sweave
+LaTeX: pdfLaTeX
+
+AutoAppendNewline: Yes
+StripTrailingWhitespace: Yes
diff --git a/image.def b/image.def
new file mode 100644
index 0000000..d9932d6
--- /dev/null
+++ b/image.def
@@ -0,0 +1,20 @@
+Bootstrap: docker
+From: r-base:4.4.4
+
+%post
+
+  # apt
+  apt-get update
+  apt-get install -y libssl-dev libxml2-dev libcurl4-openssl-dev
+
+  # Fundamental packages
+  R --slave -e 'install.packages("data.table")'
+  R --slave -e 'install.packages("R6")'
+  R --slave -e 'install.packages("bit64")'
+  R --slave -e 'install.packages("remotes")'
+  R --slave -e 'remotes::install_github("MislavSag/finfeatures")'
+  R --slave -e 'install.packages("exuber")'
+
+%runscript
+  # Script to run when the container is executed; passes commands to Rscript
+  Rscript $@
diff --git a/image.sh b/image.sh
new file mode 100644
index 0000000..dcd2d1e
--- /dev/null
+++ b/image.sh
@@ -0,0 +1,3 @@
+#!/bin/bash
+
+apptainer build image.sif image.def
diff --git a/padobran_predictors.R b/padobran_predictors.R
new file mode 100644
index 0000000..2efc2a5
--- /dev/null
+++ b/padobran_predictors.R
@@ -0,0 +1,52 @@
+options(progress_enabled = FALSE)
+
+library(data.table)
+library(finfeatures)
+
+
+# paths
+if (interactive()) {
+  PATH_PRICES     = file.path("D:/strategies/exuber/prices")
+  PATH_PREDICTORS = file.path("D:/strategies/exuber/predictors")
+} else {
+  PATH_PRICES = file.path("prices")
+  PATH_PREDICTORS = file.path("predictors")
+}
+
+# Create directory if it doesnt exists
+if (!dir.exists(PATH_PREDICTORS)) {
+  dir.create(PATH_PREDICTORS)
+}
+
+# Get index
+if (interactive()) {
+  i = 1L
+} else {
+  i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+}
+
+# Get symbol
+symbols = gsub("\\.csv", "", list.files(PATH_PRICES))
+symbol_i = symbols[i]
+
+# Import Ohlcv data
+ohlcv = fread(file.path(PATH_PRICES, paste0(symbol_i, ".csv")))
+if (attr(ohlcv$date, "tzone") == "UTC") {
+  attr(ohlcv$date, "tzone") <- "America/New_York"
+}
+# tail(ohlcv, 15)
+ohlcv[symbol == "a"]
+head(ohlcv, 20)
+ohlcv = Ohlcv$new(ohlcv[, .(symbol, date, open, high, low, close, volume)],
+                  date_col = "date")
+
+# Exuber
+exuber_init = RollingExuber$new(
+  windows = c(100, 200, 400, 800),
+  workers = 1L,
+  at = 1:220, # nrow(ohlcv$X),
+  lag = 0L,
+  exuber_lag = c(1L)
+)
+exuber = exuber_init$get_rolling_features(ohlcv, log_prices = TRUE)
+fwrite(exuber, file.path(PATH_PREDICTORS, paste0(symbol_i, ".csv")))
diff --git a/padobran_predictors.sh b/padobran_predictors.sh
new file mode 100644
index 0000000..9da16cb
--- /dev/null
+++ b/padobran_predictors.sh
@@ -0,0 +1,12 @@
+#!/bin/bash
+
+#PBS -N exuber_predictions
+#PBS -l ncpus=1
+#PBS -l mem=4GB
+#PBS -J 1-7105
+#PBS -o logs
+#PBS -j oe
+
+cd ${PBS_O_WORKDIR}
+
+apptainer run image.sif padobran_predictors.R
diff --git a/prepare.R b/prepare.R
new file mode 100644
index 0000000..cd2ba7a
--- /dev/null
+++ b/prepare.R
@@ -0,0 +1,77 @@
+library(data.table)
+library(finutils)
+
+
+# Setup
+PATH_PRICES = "D:/strategies/exuber"
+
+# Import daily data and find 1000 most liquid stocks every month
+prices = qc_daily(
+  file_path = "F:/lean/data/stocks_daily.csv",
+  min_obs = 252,
+  duplicates = "fast",
+  profiles_fmp = TRUE,
+  fmp_api_key = Sys.getenv("APIKEY")
+)
+
+# Remove ETF's
+prices = prices[isEtf == FALSE & isFund == FALSE]
+
+# Downsample to monthly data and keep most liquid symbols by month
+prices[, month := data.table::yearmon(date)]
+prices[, dollar_volume := volume * close_raw]
+pricesm = prices[, .(
+  dollar_volume = sum(dollar_volume)
+), by = .(symbol, month)]
+pricesm[, dv_rank := frankv(dollar_volume, order = -1L, ties.method = "first"), by = month]
+pricesm[month == 2025][order(dv_rank)]
+
+# Extract symbols
+symbols = pricesm[dv_rank <= 2000, unique(symbol)]
+length(symbols)
+length(symbols) / prices[, uniqueN(symbol)]
+
+# Remove prices to free memory
+rm(list = c("prices", "pricesm"))
+gc()
+
+# Import prices
+prices = qc_hour(
+  file_path = "F:/lean/data/stocks_hour.csv",
+  symbols = symbols,
+  duplicates = "fast"
+)
+
+# Save every symbol separately
+prices_dir = file.path(PATH_PRICES, "prices")
+if (!dir.exists(prices_dir)) {
+  dir.create(prices_dir)
+}
+for (s in prices[, unique(symbol)]) {
+  print(s)
+  if (s == "prn") next()
+  prices_ = prices[symbol == s]
+  if (nrow(prices_) == 0) next
+  file_name = file.path(prices_dir, paste0(s, ".csv"))
+  fwrite(prices_, file_name)
+}
+
+# Create sh file for predictors
+cont = sprintf(
+  "#!/bin/bash
+
+#PBS -N exuber_predictions
+#PBS -l ncpus=1
+#PBS -l mem=4GB
+#PBS -J 1-%d
+#PBS -o logs
+#PBS -j oe
+
+cd ${PBS_O_WORKDIR}
+
+apptainer run image.sif padobran_predictors.R",
+  length(list.files(prices_dir)))
+writeLines(cont, "padobran_predictors.sh")
+
+# Add to padobran
+# scp -r /home/sn/data/strategies/pead/prices padobran:/home/jmaric/peadml/prices
